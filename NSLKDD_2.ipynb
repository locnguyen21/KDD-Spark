{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Khai báo các thư viện để xử lý dữ liệu\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, when, max, min, sum, log, log10, log2\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, ChiSqSelector\n",
    "from pyspark.ml.classification import LinearSVC, RandomForestClassifier, LogisticRegression, NaiveBayes\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import ldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/16 19:03:06 WARN TaskSetManager: Stage 15 contains a task of very large size (10740 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/08/16 19:03:07 WARN TaskSetManager: Stage 18 contains a task of very large size (10740 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+--------+----+---------+---------+----+--------------+------+----+-----------------+---------+---------------+----------+------------+--------+------------------+----------+----------------+-----------------+-------------+--------------+-----+---------+-----------+---------------+-----------+---------------+-------------+-------------+------------------+--------------+------------------+----------------------+----------------------+---------------------------+---------------------------+--------------------+------------------------+--------------------+------------------------+-------+\n",
      "|duration|protocol_type| service|flag|src_bytes|dst_bytes|land|wrong_fragment|urgent| hot|num_failed_logins|logged_in|num_compromised|root_shell|su_attempted|num_root|num_file_creations|num_shells|num_access_files|num_outbound_cmds|is_host_login|is_guest_login|count|srv_count|serror_rate|srv_serror_rate|rerror_rate|srv_rerror_rate|same_srv_rate|diff_srv_rate|srv_diff_host_rate|dst_host_count|dst_host_srv_count|dst_host_same_srv_rate|dst_host_diff_srv_rate|dst_host_same_src_port_rate|dst_host_srv_diff_host_rate|dst_host_serror_rate|dst_host_srv_serror_rate|dst_host_rerror_rate|dst_host_srv_rerror_rate|  class|\n",
      "+--------+-------------+--------+----+---------+---------+----+--------------+------+----+-----------------+---------+---------------+----------+------------+--------+------------------+----------+----------------+-----------------+-------------+--------------+-----+---------+-----------+---------------+-----------+---------------+-------------+-------------+------------------+--------------+------------------+----------------------+----------------------+---------------------------+---------------------------+--------------------+------------------------+--------------------+------------------------+-------+\n",
      "|     0.0|          tcp|    http|  SF|    191.0|   1392.0|   0|           0.0|   0.0| 0.0|              0.0|        1|            0.0|       0.0|         0.0|     0.0|               0.0|       0.0|             0.0|              0.0|            0|             0|  1.0|      3.0|        0.0|            0.0|        0.0|            0.0|          1.0|          0.0|              0.67|           1.0|             255.0|                   1.0|                   0.0|                        1.0|                       0.05|                 0.0|                     0.0|                 0.0|                    0.01| normal|\n",
      "|     0.0|          tcp|     X11| REJ|      0.0|      0.0|   0|           0.0|   0.0| 0.0|              0.0|        0|            0.0|       0.0|         0.0|     0.0|               0.0|       0.0|             0.0|              0.0|            0|             0|  3.0|      2.0|        0.0|            0.0|       0.67|            1.0|         0.67|         0.67|               0.0|         116.0|               2.0|                  0.02|                  0.03|                       0.01|                        0.0|                 0.0|                     0.0|                0.02|                     1.0| normal|\n",
      "|     0.0|          tcp|    http|  SF|    331.0|   1483.0|   0|           0.0|   0.0| 0.0|              0.0|        1|            0.0|       0.0|         0.0|     0.0|               0.0|       0.0|             0.0|              0.0|            0|             0|  2.0|      2.0|        0.0|            0.0|        0.0|            0.0|          1.0|          0.0|               0.0|          90.0|             255.0|                   1.0|                   0.0|                       0.01|                       0.01|                 0.0|                     0.0|                 0.0|                     0.0| normal|\n",
      "|     0.0|          tcp|  telnet|  S0|      0.0|      0.0|   0|           0.0|   0.0| 0.0|              0.0|        0|            0.0|       0.0|         0.0|     0.0|               0.0|       0.0|             0.0|              0.0|            0|             0| 15.0|     15.0|        1.0|            1.0|        0.0|            0.0|          1.0|          0.0|               0.0|         177.0|              97.0|                  0.55|                  0.01|                       0.01|                        0.0|                0.99|                    0.98|                 0.0|                     0.0|anomaly|\n",
      "|     0.0|          udp|domain_u|  SF|     44.0|    115.0|   0|           0.0|   0.0| 0.0|              0.0|        0|            0.0|       0.0|         0.0|     0.0|               0.0|       0.0|             0.0|              0.0|            0|             0|  3.0|      8.0|        0.0|            0.0|        0.0|            0.0|          1.0|          0.0|              0.25|         255.0|             234.0|                  0.92|                  0.01|                        0.0|                        0.0|                 0.0|                     0.0|                 0.0|                     0.0| normal|\n",
      "|     0.0|          udp|domain_u|  SF|     35.0|     95.0|   0|           0.0|   0.0| 0.0|              0.0|        0|            0.0|       0.0|         0.0|     0.0|               0.0|       0.0|             0.0|              0.0|            0|             0|  3.0|      8.0|        0.0|            0.0|        0.0|            0.0|         0.67|         0.67|              0.25|          76.0|              50.0|                  0.66|                  0.04|                       0.01|                        0.0|                 0.0|                     0.0|                 0.0|                     0.0| normal|\n",
      "|     0.0|          tcp|ftp_data|  SF|     12.0|      0.0|   0|           0.0|   0.0| 0.0|              0.0|        1|            0.0|       0.0|         0.0|     0.0|               0.0|       0.0|             0.0|              0.0|            0|             0|  1.0|      1.0|        0.0|            0.0|        0.0|            0.0|          1.0|          0.0|               0.0|         200.0|              48.0|                  0.15|                  0.03|                       0.15|                       0.04|                 0.0|                     0.0|                 0.0|                     0.0| normal|\n",
      "|     0.0|          tcp|    http|  SF|    209.0|    803.0|   0|           0.0|   0.0| 0.0|              0.0|        1|            0.0|       0.0|         0.0|     0.0|               0.0|       0.0|             0.0|              0.0|            0|             0| 17.0|     17.0|        0.0|            0.0|        0.0|            0.0|          1.0|          0.0|               0.0|          84.0|             255.0|                   1.0|                   0.0|                       0.01|                       0.01|                0.01|                     0.0|                 0.0|                     0.0| normal|\n",
      "|     0.0|          tcp|  supdup|  S0|      0.0|      0.0|   0|           0.0|   0.0| 0.0|              0.0|        0|            0.0|       0.0|         0.0|     0.0|               0.0|       0.0|             0.0|              0.0|            0|             0|245.0|     20.0|        1.0|            1.0|        0.0|            0.0|         0.08|         0.07|               0.0|         255.0|              20.0|                  0.08|                  0.07|                        0.0|                        0.0|                 1.0|                     1.0|                 0.0|                     0.0|anomaly|\n",
      "|     0.0|          tcp|    http|  SF|    228.0|   8550.0|   0|           0.0|   0.0| 0.0|              0.0|        1|            0.0|       0.0|         0.0|     0.0|               0.0|       0.0|             0.0|              0.0|            0|             0|  3.0|      3.0|        0.0|            0.0|        0.0|            0.0|          1.0|          0.0|               0.0|           3.0|             255.0|                   1.0|                   0.0|                       0.33|                       0.05|                 0.0|                     0.0|                 0.0|                     0.0| normal|\n",
      "|    27.0|          tcp|     ftp|  SF|   1483.0|   4152.0|   0|           0.0|   0.0|30.0|              0.0|        1|            0.0|       0.0|         0.0|     0.0|               0.0|       0.0|             0.0|              0.0|            0|             1|  1.0|      1.0|        0.0|            0.0|        0.0|            0.0|          1.0|          0.0|               0.0|          44.0|              12.0|                  0.27|                  0.11|                       0.02|                        0.0|                0.02|                     0.0|                 0.0|                     0.0| normal|\n",
      "|     0.0|          tcp|    time|RSTR|      0.0|      0.0|   0|           0.0|   0.0| 0.0|              0.0|        0|            0.0|       0.0|         0.0|     0.0|               0.0|       0.0|             0.0|              0.0|            0|             0|  1.0|      1.0|        0.0|            0.0|        1.0|            1.0|          1.0|          0.0|               0.0|         255.0|               1.0|                   0.0|                  0.89|                       0.88|                        0.0|                 0.0|                     0.0|                0.88|                     1.0|anomaly|\n",
      "|     0.0|          tcp|ftp_data|  SF|    749.0|      0.0|   0|           0.0|   0.0| 0.0|              0.0|        1|            0.0|       0.0|         0.0|     0.0|               0.0|       0.0|             0.0|              0.0|            0|             0|  5.0|      5.0|        0.0|            0.0|        0.0|            0.0|          1.0|          0.0|               0.0|         255.0|             125.0|                  0.49|                  0.02|                       0.49|                        0.0|                 0.0|                     0.0|                 0.0|                     0.0| normal|\n",
      "|     0.0|          tcp|    http|  SF|    323.0|   4374.0|   0|           0.0|   0.0| 0.0|              0.0|        1|            0.0|       0.0|         0.0|     0.0|               0.0|       0.0|             0.0|              0.0|            0|             0|  4.0|      8.0|        0.0|            0.0|        0.0|            0.0|          1.0|          0.0|              0.25|           5.0|             255.0|                   1.0|                   0.0|                        0.2|                       0.03|                 0.0|                     0.0|                 0.0|                     0.0| normal|\n",
      "|     0.0|          tcp|    http|  SF|    302.0|   1103.0|   0|           0.0|   0.0| 0.0|              0.0|        1|            0.0|       0.0|         0.0|     0.0|               0.0|       0.0|             0.0|              0.0|            0|             0|  3.0|      3.0|        0.0|            0.0|        0.0|            0.0|          1.0|          0.0|               0.0|          89.0|             255.0|                   1.0|                   0.0|                       0.01|                       0.02|                 0.0|                     0.0|                 0.0|                     0.0| normal|\n",
      "|     0.0|          tcp|    http|  SF|    326.0|   1770.0|   0|           0.0|   0.0| 0.0|              0.0|        1|            0.0|       0.0|         0.0|     0.0|               0.0|       0.0|             0.0|              0.0|            0|             0|  8.0|     15.0|        0.0|            0.0|        0.0|            0.0|          1.0|          0.0|              0.13|          27.0|             255.0|                   1.0|                   0.0|                       0.04|                       0.09|                 0.0|                     0.0|                 0.0|                     0.0| normal|\n",
      "|     0.0|          tcp| daytime|  S0|      0.0|      0.0|   0|           0.0|   0.0| 0.0|              0.0|        0|            0.0|       0.0|         0.0|     0.0|               0.0|       0.0|             0.0|              0.0|            0|             0|147.0|     16.0|        1.0|            1.0|        0.0|            0.0|         0.11|         0.07|               0.0|         255.0|              16.0|                  0.06|                  0.07|                        0.0|                        0.0|                 1.0|                     1.0|                 0.0|                     0.0|anomaly|\n",
      "|     0.0|          tcp| private|  S0|      0.0|      0.0|   0|           0.0|   0.0| 0.0|              0.0|        0|            0.0|       0.0|         0.0|     0.0|               0.0|       0.0|             0.0|              0.0|            0|             0|221.0|     19.0|        1.0|            1.0|        0.0|            0.0|         0.09|         0.06|               0.0|         255.0|              17.0|                  0.07|                  0.06|                        0.0|                        0.0|                 1.0|                     1.0|                 0.0|                     0.0|anomaly|\n",
      "|     0.0|          udp| private|  SF|    215.0|      0.0|   0|           0.0|   0.0| 0.0|              0.0|        0|            0.0|       0.0|         0.0|     0.0|               0.0|       0.0|             0.0|              0.0|            0|             0|  1.0|      1.0|        0.0|            0.0|        0.0|            0.0|          1.0|          0.0|               0.0|         171.0|             147.0|                  0.86|                  0.01|                       0.86|                        0.0|                 0.0|                     0.0|                 0.0|                     0.0|anomaly|\n",
      "|     0.0|          tcp| sql_net|RSTR|      0.0|      0.0|   0|           0.0|   0.0| 0.0|              0.0|        0|            0.0|       0.0|         0.0|     0.0|               0.0|       0.0|             0.0|              0.0|            0|             0|  1.0|      1.0|        0.0|            0.0|        1.0|            1.0|          1.0|          0.0|               0.0|         255.0|               1.0|                   0.0|                   1.0|                        1.0|                        0.0|                 0.0|                     0.0|                 1.0|                     1.0|anomaly|\n",
      "+--------+-------------+--------+----+---------+---------+----+--------------+------+----+-----------------+---------+---------------+----------+------------+--------+------------------+----------+----------------+-----------------+-------------+--------------+-----+---------+-----------+---------------+-----------+---------------+-------------+-------------+------------------+--------------+------------------+----------------------+----------------------+---------------------------+---------------------------+--------------------+------------------------+--------------------+------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Tổng số lượng record: 125973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/16 19:03:08 WARN TaskSetManager: Stage 24 contains a task of very large size (10740 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 24:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|  class|count|\n",
      "+-------+-----+\n",
      "| normal|67343|\n",
      "|anomaly|58630|\n",
      "+-------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Load dữ liệu arff sang pandas Dataframe\n",
    "file_path= '/root/NSLKDDProject/nslkdd/KDDTrain+.arff'\n",
    "df = ldata.load_arff_to_pandasDF(file_path)\n",
    "#print(df)\n",
    "# Khởi tạo Spark session\n",
    "spark = SparkSession.builder.appName(\"NSLKDDApp\").getOrCreate()\n",
    "#Load DataFrame\n",
    "spark_df = spark.createDataFrame(df)\n",
    "spark_df = spark_df.repartition(100)\n",
    "#Show dữ liệu \n",
    "spark_df.show()\n",
    "total_records = spark_df.count()\n",
    "print(f\"Tổng số lượng record: {total_records}\")\n",
    "#Đếm số lượng record của mỗi class\n",
    "class_counts = spark_df.groupBy(\"class\").count()\n",
    "class_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mô tả dữ liệu (statstics cho mỗi cột)\n",
    "# count: Kiểm tra giá trị không null\n",
    "# mean: Giá trị trung bình\n",
    "# stddev: Độ lệch chuẩn\n",
    "# min: giá trị lớn nhất\n",
    "# max: Giá trị nhỏ nhất\n",
    "description = spark_df.describe()\n",
    "description.show()\n",
    "spark_df.printSchema()\n",
    "column_types = spark_df.dtypes\n",
    "# Print the data type    \n",
    "type_counts = {}\n",
    "for _, dtype in column_types:\n",
    "    if dtype in type_counts:\n",
    "        type_counts[dtype] += 1\n",
    "    else:\n",
    "        type_counts[dtype] = 1\n",
    "\n",
    "# Đếm giá trị các trường dữ liệu\n",
    "for dtype, count in type_counts.items():\n",
    "    print(f\"Data Type: {dtype}, Count: {count}\")\n",
    "print(\"\")  \n",
    "for column, dtype in column_types:\n",
    "    if (dtype == 'string'):\n",
    "        print(f\"Column: {column}, Type: {dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xem xét một số trường dữ liệu \n",
    "#protocol_type: giao thức trong lưu lượng mạng\n",
    "protocol_counts = spark_df.groupBy(\"protocol_type\").count()\n",
    "protocol_counts.show()\n",
    "#service: dịch vụ truyền tải trong lưu lượng mạng\n",
    "service_counts = spark_df.groupBy(\"service\").count()\n",
    "service_counts.show()\n",
    "#flag: trạng thái cờ trong lưu lượng mạng theo mô hình TCP/IP\n",
    "flag_counts = spark_df.groupBy(\"flag\").count()\n",
    "flag_counts.show()\n",
    "#service_percentage = service_counts.withColumn(\"Percentage\", (col(\"service\") / total_records) * 100)\n",
    "#service_percentage.show()\n",
    "\n",
    "#Kiểm tra giá trị null trên dữ liệu\n",
    "#null_counts = {}\n",
    "#for checkcol in spark_df.columns:\n",
    "#    null_count = spark_df.filter(spark_df[checkcol].isNull()).count()\n",
    "#    null_counts[checkcol] = null_count\n",
    "#print(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THỰC HIỆN PREPROCESSING DATA BAO GỒM\n",
    "#0. Drop cột\n",
    "spark_df = spark_df.drop(\"num_outbound_cmds\")\n",
    "#1. One hot encoding data string dựa trên tần suất xuất hiện của value trong data\n",
    "translist = [\"protocol_type\", \"service\", \"flag\"]\n",
    "translist_temp = [\"protocol_type_trans\", \"service_trans\", \"flag_trans\"]\n",
    "for i in range(len(translist)):\n",
    "    indexer = StringIndexer(inputCol=translist[i], outputCol=translist_temp[i])\n",
    "    spark_df = indexer.fit(spark_df).transform(spark_df)\n",
    "    spark_df = spark_df.drop(translist[i]).withColumnRenamed(translist_temp[i], translist[i])\n",
    "\n",
    "#2. Chuyển đổi các cột string cast về double\n",
    "castlistvalue = [\"land\", \"logged_in\", \"is_host_login\", \"is_guest_login\"]\n",
    "for i in range (len(castlistvalue)):\n",
    "    spark_df = spark_df.withColumn(castlistvalue[i], col(castlistvalue[i]).cast(DoubleType()))\n",
    "\n",
    "#3 Logarithmic scaling method dành cho các trường dữ liệu có độ chênh lệch lớn giữa max và min value\n",
    "#loglist = [\"duration\", \"src_bytes\", \"dst_bytes\", \"num_compromised\",\"num_root\"]\n",
    "#loglist_temp = [\"duration_trans\", \"src_bytes_trans\", \"dst_bytes_trans\", \"num_compromised_trans\",\"num_root_trans\"]\n",
    "loglist = [\"duration\", \"src_bytes\", \"dst_bytes\"]\n",
    "loglist_temp = [\"duration_trans\", \"src_bytes_trans\", \"dst_bytes_trans\"]\n",
    "for i in range(len(loglist)):\n",
    "    #spark_df = spark_df.withColumn(loglist_temp[i], log2(loglist[i]))\n",
    "    spark_df = spark_df.withColumn(loglist_temp[i],when(col(loglist[i]) > 0, log(loglist[i])).otherwise(0))\n",
    "    spark_df = spark_df.drop(loglist[i]).withColumnRenamed(loglist_temp[i], loglist[i])\n",
    "\n",
    "#4 Chuyển đổi label data thành 0,1 \n",
    "class_name = {\"normal\": \"0\", \"anomaly\": \"1\"}\n",
    "spark_df = spark_df.replace(class_name, subset=[\"class\"])\n",
    "spark_df = spark_df.withColumn(\"class\", col(\"class\").cast(DoubleType()))\n",
    "#class_df = spark_df.select(\"class\")\n",
    "#spark_df = spark_df.drop(\"class\")\n",
    "#5 Normalize Data\n",
    "print(\"Sau khi scalling\")\n",
    "spark_df.show()\n",
    "columntypes = spark_df.dtypes\n",
    "for column, dtype in columntypes:\n",
    "    sum_value = spark_df.agg(sum(col(column))).first()[0]\n",
    "    if (sum_value == 0):\n",
    "        print(column)\n",
    "    elif (column == \"class\"):\n",
    "        continue\n",
    "    elif (column in translist):\n",
    "        continue\n",
    "    else:\n",
    "        min_value = spark_df.select(min(col(column))).first()[0]\n",
    "        max_value = spark_df.select(max(col(column))).first()[0]\n",
    "        spark_df = spark_df.withColumn(column,(col(column) - min_value) / (max_value - min_value))\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kiểm tra dữ liệu sau khi thực hiện tiền xử lý trước khi đưa vào học máy\n",
    "#spark_df.show()\n",
    "null_counts = {}\n",
    "for checkcol in spark_df.columns:\n",
    "    null_count = spark_df.filter(spark_df[checkcol].isNull()).count()\n",
    "    null_counts[checkcol] = null_count\n",
    "print(null_counts)\n",
    "description_after = spark_df.describe()\n",
    "description_after.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_types1 = spark_df.dtypes\n",
    "inputColumns = []\n",
    "#outputCols = []\n",
    "for column, dtype in columntypes:\n",
    "    if (column == \"class\"):\n",
    "        continue\n",
    "    else:\n",
    "        inputColumns.append(column)\n",
    "print(inputColumns)\n",
    "#print(outputCols)\n",
    "\n",
    "assembler = VectorAssembler(inputCols=inputColumns, outputCol= \"features\")\n",
    "\n",
    "#df_assembled = assembler.transform(spark_df).select(\"class\", \"features\")\n",
    "df_assembled = assembler.transform(spark_df)\n",
    "selector = ChiSqSelector(numTopFeatures=10, featuresCol=\"features\", outputCol=\"selected_features\", labelCol=\"class\")\n",
    "selectorModel = selector.fit(df_assembled)\n",
    "df_selected = selectorModel.transform(df_assembled)\n",
    "# Prepare DataFrame for model training\n",
    "#df_final = df_selected.select(\"selected_features\", \"class\")\n",
    "#df_final.show()\n",
    "\n",
    "#df_assembled = assembler.transform(spark_df).select(\"class\", \"features\")\n",
    "##df_assembled = assembler.transform(spark_df)\n",
    "##df_final = df_assembled.select(\"features\", \"class\")\n",
    "#df_final.show()\n",
    "#train_data, test_data = df_final.randomSplit([0.8, 0.2], seed=42)\n",
    "train_data, test_data = spark_df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes(featuresCol=inputColumns, labelCol=\"class\")\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(nb.smoothing, [0.0, 0.5, 1.0])\n",
    "             .addGrid(nb.modelType, [\"bernoulli\"])\n",
    "             .build())\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"class\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "crossval = CrossValidator(estimator=nb,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3)\n",
    "cvModel = crossval.fit(train_data)\n",
    "\n",
    "# Make predictions on test set\n",
    "predictions = cvModel.transform(test_data)\n",
    "predictions.select(inputColumns, \"class\", \"prediction\", \"probability\").show()\n",
    "\n",
    "# Evaluate best model\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Do chinh xac tren tap du lieu mau = {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol=\"selected_features\", labelCol=\"class\")\n",
    "\n",
    "# Optional: Hyperparameter tuning\n",
    "#paramGrid = ParamGridBuilder().addGrid(lr.regParam, [0.1, 0.01]).build()\n",
    "#crossval = CrossValidator(estimator=lr,\n",
    "#                          estimatorParamMaps=paramGrid,\n",
    "#                          evaluator=BinaryClassificationEvaluator(),\n",
    "#                          numFolds=5)\n",
    "\n",
    "# Train model\n",
    "cVmodel = lr.fit(train_data)\n",
    "predictions = cVmodel.transform(test_data)\n",
    "predictions.select(\"selected_features\", \"class\", \"prediction\", \"probability\").show()\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",labelCol=\"class\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Do chinh xac tren tap du lieu mau = {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"class\")\n",
    "\n",
    "# Optional: Hyperparameter tuning\n",
    "paramGrid = ParamGridBuilder().addGrid(rf.numTrees, [10, 20]).build()\n",
    "crossval = CrossValidator(estimator=rf,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=5)\n",
    "\n",
    "# Train model\n",
    "cVmodel = crossval.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = cVmodel.transform(test_data)\n",
    "predictions.select(\"class\", \"features\", \"prediction\").show()\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"class\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Do chinh xac tren tap du lieu mau = {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n",
    "svm = LinearSVC(maxIter=10, regParam=0.1, labelCol=\"class\", featuresCol=\"features\")\n",
    "cvModel = svm.fit(train_data)\n",
    "predictions = cvModel.transform(test_data)\n",
    "predictions.select(\"class\", \"features\", \"prediction\").show()\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"class\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Do chinh xac tren tap du lieu mau = {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training SVM model\n",
    "#Gộp các cột thành 1 vector cột\n",
    "column_types1 = spark_df.dtypes\n",
    "inputColumns = []\n",
    "#outputCols = []\n",
    "for column, dtype in columntypes:\n",
    "    if (column == \"class\"):\n",
    "        continue\n",
    "    else:\n",
    "        inputColumns.append(column)\n",
    "print(inputColumns)\n",
    "#print(outputCols)\n",
    "\n",
    "assembler = VectorAssembler(inputCols=inputColumns, outputCol= \"features\")\n",
    "#df_assembled = assembler.transform(spark_df).select(\"class\", \"features\")\n",
    "df_assembled = assembler.transform(spark_df)\n",
    "selector = ChiSqSelector(numTopFeatures=10, featuresCol=\"features\", outputCol=\"selected_features\", labelCol=\"class\")\n",
    "selectorModel = selector.fit(df_assembled)\n",
    "df_selected = selectorModel.transform(df_assembled)\n",
    "# Prepare DataFrame for model training\n",
    "df_final = df_selected.select(\"selected_features\", \"class\")\n",
    "df_final.show()\n",
    "\n",
    "train_data, test_data = df_final.randomSplit([0.8, 0.2], seed=1234)\n",
    "\n",
    "#svm = LinearSVC(maxIter=10, regParam=0.1, labelCol=\"class\", featuresCol=\"selected_features\")\n",
    "#cvModel = svm.fit(train_data)\n",
    "#predictions = cvModel.transform(test_data)\n",
    "svm = LinearSVC(labelCol=\"class\", featuresCol=\"selected_features\")\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(svm.regParam, [0.01, 0.1, 1.0]) \\\n",
    "    .addGrid(svm.maxIter, [10, 50, 100]) \\\n",
    "    .build()\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"class\", rawPredictionCol=\"rawPrediction\")\n",
    "crossval = CrossValidator(estimator=svm,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5)\n",
    "cvModel = crossval.fit(train_data)\n",
    "#cvModel = crossval.fit(df_assembled)\n",
    "predictions = cvModel.transform(test_data)\n",
    "\n",
    "#predictions.select(\"class\", \"features\", \"prediction\").show()\n",
    "predictions.select(\"class\", \"selected_features\", \"prediction\").show()\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"class\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Do chinh xac tren tap du lieu mau = {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sau khi training model xong, thuc hien tren data tu file Test\n",
    "file_path= '/root/NSLKDDProject/nslkdd/KDDTest+.arff'\n",
    "df_test = ldata.load_arff_to_pandasDF(file_path)\n",
    "#Load DataFrame\n",
    "spark_df_test = spark.createDataFrame(df_test)\n",
    "#Show dữ liệu \n",
    "spark_df_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THỰC HIỆN PREPROCESSING DATA BAO GỒM\n",
    "#0. Drop cột\n",
    "spark_df_test = spark_df_test.drop(\"num_outbound_cmds\")\n",
    "#1. One hot encoding data string dựa trên tần suất xuất hiện của value trong data\n",
    "translist = [\"protocol_type\", \"service\", \"flag\"]\n",
    "translist_temp = [\"protocol_type_trans\", \"service_trans\", \"flag_trans\"]\n",
    "for i in range(len(translist)):\n",
    "    indexer1 = StringIndexer(inputCol=translist[i], outputCol=translist_temp[i])\n",
    "    spark_df_test = indexer1.fit(spark_df_test).transform(spark_df_test)\n",
    "    spark_df_test = spark_df_test.drop(translist[i]).withColumnRenamed(translist_temp[i], translist[i])\n",
    "\n",
    "#2. Chuyển đổi các cột string cast về double\n",
    "castlistvalue = [\"land\", \"logged_in\", \"is_host_login\", \"is_guest_login\"]\n",
    "for i in range (len(castlistvalue)):\n",
    "    spark_df_test = spark_df_test.withColumn(castlistvalue[i], col(castlistvalue[i]).cast(DoubleType()))\n",
    "#3 Tách label data ra Dataframe mới\n",
    "class_name = {\"normal\": \"0\", \"anomaly\": \"1\"}\n",
    "spark_df_test = spark_df_test.replace(class_name, subset=[\"class\"])\n",
    "spark_df_test = spark_df_test.withColumn(\"class\", col(\"class\").cast(DoubleType()))\n",
    "\n",
    "loglist = [\"duration\", \"src_bytes\", \"dst_bytes\"]\n",
    "loglist_temp = [\"duration_trans\", \"src_bytes_trans\", \"dst_bytes_trans\"]\n",
    "for i in range(len(loglist)):\n",
    "    #spark_df = spark_df.withColumn(loglist_temp[i], log2(loglist[i]))\n",
    "    spark_df_test = spark_df_test.withColumn(loglist_temp[i],when(col(loglist[i]) > 0, log(loglist[i])).otherwise(0))\n",
    "    spark_df_test = spark_df_test.drop(loglist[i]).withColumnRenamed(loglist_temp[i], loglist[i])\n",
    "#class_df = spark_df.select(\"class\")\n",
    "#spark_df = spark_df.drop(\"class\")\n",
    "#4 Normalize Data\n",
    "columntypes = spark_df_test.dtypes\n",
    "for column, dtype in columntypes:\n",
    "    sum_value = spark_df_test.agg(sum(col(column))).first()[0]\n",
    "    if (sum_value == 0):\n",
    "        print(column)\n",
    "    elif (column == \"class\"):\n",
    "        continue\n",
    "    elif (column in translist):\n",
    "        continue\n",
    "    else:\n",
    "        min_value = spark_df_test.select(min(col(column))).first()[0]\n",
    "        max_value = spark_df_test.select(max(col(column))).first()[0]\n",
    "        spark_df_test = spark_df_test.withColumn(column,(col(column) - min_value) / (max_value - min_value))\n",
    "spark_df_test.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "des = spark_df_test.describe()\n",
    "des.show()\n",
    "column_types1 = spark_df_test.dtypes\n",
    "inputColumns = []\n",
    "#outputCols = []\n",
    "for column, dtype in columntypes:\n",
    "    if (column == \"class\"):\n",
    "        continue\n",
    "    else:\n",
    "        inputColumns.append(column)\n",
    "print(inputColumns)\n",
    "#print(outputCols)\n",
    "\n",
    "#df_test_assembled = assembler.transform(spark_df_test).select(\"class\", \"features\")\n",
    "df_test_assembled = assembler.transform(spark_df_test)\n",
    "df_test_final = df_test_assembled.select(\"features\", \"class\")\n",
    "\n",
    "predictions = cvModel.transform(df_test_final)\n",
    "#predictions = cvModel.transform(df_test_assembled)\n",
    "#predictions.select(\"class\", \"features\", \"prediction\").show()\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"class\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Do chinh xac tren tap du lieu test = {accuracy}\")\n",
    "truep = predictions.filter((col(\"class\") == 1) & (col(\"prediction\") == 1)).count()\n",
    "truen = predictions.filter((col(\"class\") == 0) & (col(\"prediction\") == 0)).count()\n",
    "falsep = predictions.filter((col(\"class\") == 0) & (col(\"prediction\") == 1)).count()\n",
    "falsen = predictions.filter((col(\"class\") == 1) & (col(\"prediction\") == 0)).count()\n",
    "print(f\"True Positives: {truep}\")\n",
    "print(f\"True Negatives: {truen}\")\n",
    "print(f\"False Positives: {falsep}\")\n",
    "print(f\"False Negatives: {falsen}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "des = spark_df_test.describe()\n",
    "des.show()\n",
    "column_types1 = spark_df_test.dtypes\n",
    "inputColumns = []\n",
    "#outputCols = []\n",
    "for column, dtype in columntypes:\n",
    "    if (column == \"class\"):\n",
    "        continue\n",
    "    else:\n",
    "        inputColumns.append(column)\n",
    "print(inputColumns)\n",
    "#print(outputCols)\n",
    "\n",
    "#df_test_assembled = assembler.transform(spark_df_test).select(\"class\", \"features\")\n",
    "df_test_assembled = assembler.transform(spark_df_test)\n",
    "df_test_selected = selectorModel.transform(df_test_assembled)\n",
    "\n",
    "# Prepare DataFrame for making predictions\n",
    "df_test_final = df_test_selected.select(\"selected_features\", \"class\")\n",
    "\n",
    "predictions = cVmodel.transform(df_test_final)\n",
    "#predictions = cvModel.transform(df_test_assembled)\n",
    "#predictions.select(\"class\", \"features\", \"prediction\").show()\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"class\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Do chinh xac tren tap du lieu test = {accuracy}\")\n",
    "truep = predictions.filter((col(\"class\") == 1) & (col(\"prediction\") == 1)).count()\n",
    "truen = predictions.filter((col(\"class\") == 0) & (col(\"prediction\") == 0)).count()\n",
    "falsep = predictions.filter((col(\"class\") == 0) & (col(\"prediction\") == 1)).count()\n",
    "falsen = predictions.filter((col(\"class\") == 1) & (col(\"prediction\") == 0)).count()\n",
    "print(f\"True Positives: {truep}\")\n",
    "print(f\"True Negatives: {truen}\")\n",
    "print(f\"False Positives: {falsep}\")\n",
    "print(f\"False Negatives: {falsen}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
